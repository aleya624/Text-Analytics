{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab7438b-13c7-4b87-802c-bb5b75084a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/0ef5607c-92a3-4202-a7d3-\n",
      "[nltk_data]     326ceacd22cf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/0ef5607c-92a3-4202-a7d3-\n",
      "[nltk_data]     326ceacd22cf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/0ef5607c-92a3-4202-a7d3-\n",
      "[nltk_data]     326ceacd22cf/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/0ef5607c-92a3-4202-a7d3-\n",
      "[nltk_data]     326ceacd22cf/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0  Im happy with uniten actually, even the people...   \n",
      "1  Iâ€™m having a pretty good time here, happy to m...   \n",
      "2        a very neutral place in terms of everything   \n",
      "3  I would say Uniten it's  a good university  bu...   \n",
      "4   UNITEN is well-regarded, particularly for its...   \n",
      "\n",
      "                                           processed  \n",
      "0   [im, happy, unite, actually, even, people, good]  \n",
      "1  [im, pretty, good, time, happy, meet, good, pe...  \n",
      "2                 [neutral, place, term, everything]  \n",
      "3  [would, say, united, good, university, issue, ...  \n",
      "4  [united, wellregarded, particularly, strong, e...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "slang_dict = {\n",
    "    \"w\": \"good\", \"tbh\": \"to be honest\", \"imo\": \"in my opinion\",\n",
    "    \"yg\": \"yang\", \"tpi\": \"tapi\", \"tak\": \"tidak\"}\n",
    "\n",
    "def preprocess_uniten(text):\n",
    "    if not isinstance(text, str) or text == \"#NAME?\": \n",
    "     return \"\" # Fix for #NAME?\n",
    "    \n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8', 'ignore') \n",
    "    text = text.lower() \n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text) \n",
    "\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text() \n",
    "\n",
    "    text = emoji.replace_emoji(text, replace='') \n",
    "\n",
    "    words = text.split()\n",
    "    text = \" \".join([slang_dict.get(w, w) for w in words])\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    words = text.split()\n",
    "    corrected_words = [spell(w) if len(w) > 3 else w for w in words]\n",
    "    text = \" \".join(corrected_words)\n",
    "\n",
    "    words = text.split()\n",
    "    text = \" \".join([w for w in words if w not in stop_words])\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'): return wordnet.ADJ\n",
    "        elif tag.startswith('V'): return wordnet.VERB\n",
    "        elif tag.startswith('N'): return wordnet.NOUN\n",
    "        elif tag.startswith('R'): return wordnet.ADV\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(t)) for w, t in pos_tags]\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"UNITENReview.csv\")\n",
    "df[\"processed\"] = df[\"Review\"].apply(preprocess_uniten)\n",
    "df.to_csv(\"Processed_UNITENReviews.csv\", index=False)\n",
    "print(df[[\"Review\", \"processed\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee02839-fd17-4dba-bd9b-4b6e5084c3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.12-py312",
   "language": "python",
   "name": "conda-env-anaconda-2025.12-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
